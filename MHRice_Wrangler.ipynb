{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head cell\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open('breakdown/misc.json') as file:\n",
    "    raw = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monster Names & IDs\n",
    "SECTIONS:\n",
    "- **monster_names**: rise english names\n",
    "- **monster_names_mr**: sunbreak english names\n",
    "- **monsters**: IDs\n",
    "- **random_mystery_enemy**: monster A★ and M★ rank numbers\n",
    "\n",
    "COLUMNS:\n",
    "- **id**: base monster id\n",
    "- **sub_id**: indicates monster variant\n",
    "- **em_id** (enemy_type): monster id when referenced by \"EnemyIndex###(_MR)\"\n",
    "- **em_type.Em**: internal monster identifier\n",
    "- **mon_name**: monster name\n",
    "- **normal_rank**: M★ rank. 12 indicates access through anomaly only.\n",
    "- **mystery_rank**: A★ rank. 12 indicates access as secondary target only.\n",
    "\n",
    "OUTPUT: **df_monsters**\n",
    "| id | sub_id | em_id | em_type.Em | mon_name | normal_rank | mystery_rank |\n",
    "|-|-|-|-|-|-|-|\n",
    "| 1 | 0 | 0 | 1 | Rathian | 2 | 3 |\n",
    "| 1 | 2 | 76 | 513 | Gold Rathian | 6 | 7 |\n",
    "| 1 | 7 | 1 | 1793 | Apex Rathian | 12 | 12 |\n",
    "| 2 | 0 | 2 | 2 | Rathalos | 4 | 5 |\n",
    "| 2 | 2 | 77 | 514 | Silver Rathalos | 6 | 7 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_entry = True #maintaints Toadversary and non-anomaly MR monsters in data\n",
    "\n",
    "with open('breakdown/monsters.json') as file:\n",
    "    raw = json.load(file)\n",
    "\n",
    "# Monster Names\n",
    "sect = raw.get('monster_names').get('entries')\n",
    "df = pd.json_normalize(sect)\n",
    "df['mon_name'] = df['content'].apply(lambda x: x[1]) #grab only english name\n",
    "df['em_id'] = pd.to_numeric(df['name'].apply(lambda x: x[-3:])) #grab only id number\n",
    "df = df.iloc[0:46] #remove small monsters\n",
    "df_monster_names = df[['em_id', 'mon_name']].copy()\n",
    "\n",
    "# MR Monster Names\n",
    "sect = raw.get('monster_names_mr').get('entries')\n",
    "df = pd.json_normalize(sect)\n",
    "df['mon_name'] = df['content'].apply(lambda x: x[1])\n",
    "df['em_id'] = pd.to_numeric(df['name'].apply(lambda x: x[-6:-3]))\n",
    "df = pd.concat([df.iloc[0:23], df.iloc[31:]], axis=0, ignore_index=True)\n",
    "\n",
    "df_monster_names = pd.concat([df_monster_names, df[['em_id', 'mon_name']]], axis=0, ignore_index=True)\n",
    "\n",
    "# Monster IDs\n",
    "sect = raw.get('monsters')\n",
    "df = pd.json_normalize(sect)\n",
    "df = df[['id','sub_id','enemy_type','em_type.Em']]\n",
    "df.rename(columns={'enemy_type':'em_id'}, inplace=True)\n",
    "\n",
    "df_monsters = pd.merge(df,df_monster_names, on='em_id', how='left')\n",
    "\n",
    "if manual_entry:\n",
    "    df_monsters.loc[df_monsters['em_id'] == 46, 'mon_name'] = 'Toadversary' # add Toadversary to list\n",
    "else:\n",
    "    df_monsters.dropna(subset=['mon_name'], inplace=True) # drop Toadversary from list\n",
    "    df_monsters.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Anomaly Data\n",
    "sect = raw.get('random_mystery_enemy').get('lot_enemy_list')\n",
    "df = pd.json_normalize(sect)\n",
    "df = df[['em_type.Em','normal_rank','mystery_rank']]\n",
    "\n",
    "df_monsters = pd.merge(df_monsters,df, on='em_type.Em', how='left')\n",
    "\n",
    "if manual_entry:\n",
    "    df_monsters.loc[df_monsters['mon_name'] == 'Amatsu', 'normal_rank'] = 6\n",
    "    df_monsters.loc[df_monsters['mon_name'] == 'Wind Serpent Ibushi', 'normal_rank'] = 6\n",
    "    df_monsters.loc[df_monsters['mon_name'] == 'Thunder Serpent Narwa', 'normal_rank'] = -1\n",
    "    df_monsters.loc[df_monsters['mon_name'] == 'Narwa the Allmother', 'normal_rank'] = 6\n",
    "    df_monsters.loc[df_monsters['mon_name'] == 'Gaismagorm', 'normal_rank'] = 6\n",
    "else:\n",
    "    df_monsters.dropna(subset=['mystery_rank'], inplace=True)\n",
    "    df_monsters.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_monsters.fillna(-1, inplace=True)\n",
    "df_monsters[['normal_rank','mystery_rank']] = df_monsters[['normal_rank','mystery_rank']].astype(int)\n",
    "\n",
    "df_monsters.to_excel('monster.xlsx')\n",
    "# print(df_monsters.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item Names, IDs, & Properties\n",
    "SECTIONS:\n",
    "- **items_name_msg**: rise english names & ids\n",
    "- **items_name_msg_mr**: sunbreak english names & ids\n",
    "- **items**: item properties\n",
    "\n",
    "OUTPUT: **df_items**\n",
    "| item_id | item_name | type_ | category_prefix | material_category | category_worth | rare |\n",
    "|-|-|-|-|-|-|-|\n",
    "| 6 | Potion | Consume | 'None' | [] | 0 | 1 |\n",
    "| 7 | Mega Potion | Consume | 'None' | [] | 0 | 2 |\n",
    "| 5 | Max Potion | Consume | 'None' | [] | 0 | 3 |\n",
    "| 467 | Ancient Potion | Consume | 'None' | [] | 0 | 4 |\n",
    "| 13 | Antidote | Consume | 'None' | [] | 0 | 1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('breakdown/items.json') as file:\n",
    "    raw = json.load(file)\n",
    "\n",
    "# Item Names & IDs\n",
    "sect = raw.get('items_name_msg').get('entries')\n",
    "df = pd.json_normalize(sect)\n",
    "df.rename(columns={'name':'item_id'}, inplace=True)\n",
    "df=df[~df['item_id'].str.contains('I_EC_')] # remove endemic life\n",
    "df['item_id'] = df['item_id'].str.extract('(\\d+)', expand=False) # grab only item number\n",
    "df.dropna(inplace=True) # drop blanks\n",
    "df['item_id'] = pd.to_numeric(df['item_id'], errors='coerce', downcast='integer') # convert column to int\n",
    "df['item_name'] = df['content'].apply(lambda x: x[1]) # grab only english name\n",
    "df = df[~df['item_name'].str.contains('COLOR FF0000', na=False)] # remove placeholder items\n",
    "df_item_names = df[['item_id', 'item_name']].copy()\n",
    "\n",
    "# MR Item Names & IDs\n",
    "sect = raw.get('items_name_msg_mr').get('entries')\n",
    "df = pd.json_normalize(sect)\n",
    "df.rename(columns={'name':'item_id'}, inplace=True)\n",
    "df=df[~df['item_id'].str.contains('I_EC_')]\n",
    "df['item_id'] = df['item_id'].str.extract('(\\d+)', expand=False)\n",
    "df.dropna(inplace=True)\n",
    "df['item_id'] = pd.to_numeric(df['item_id'], errors='coerce', downcast='integer')\n",
    "df['item_name'] = df['content'].apply(lambda x: x[1])\n",
    "df = df[~df['item_name'].str.contains('COLOR FF0000', na=False)]\n",
    "\n",
    "df_item_names = pd.concat((df_item_names, df[['item_id', 'item_name']]), axis=0, ignore_index=True)\n",
    "\n",
    "# Item Types, Categories, Worth, and Rarity\n",
    "sect = raw.get('items').get('param')\n",
    "df = pd.json_normalize(sect)\n",
    "df.rename(columns={'id.Normal':'item_id'}, inplace=True)\n",
    "df = df[['item_id', 'type_', 'material_category', 'category_worth', 'rare']]\n",
    "df = pd.merge(df, df_item_names, on='item_id', how='left')\n",
    "df.dropna(subset=['item_name'], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Denesting Category Dictionary\n",
    "df['category_prefix'] = df['material_category'].apply(lambda x: next(iter(x[0])) if isinstance(x[0],dict) else x[0]) # prefix\n",
    "df['material_category'] = df['material_category'].apply(lambda x: [d for d in x if isinstance(d, dict)]) # None removal\n",
    "df['material_category'] = df['material_category'].apply(lambda x: [list(d.values())[0] for d in x] if x else []) # denest\n",
    "\n",
    "df_items = df[['item_id', 'item_name', 'type_', 'category_prefix', 'material_category', 'category_worth', 'rare']].copy()\n",
    "\n",
    "print(df_items.head())\n",
    "# df_items.to_excel('items.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Tables\n",
    "SECTIONS:\n",
    "- **reward_id_lot_table**: base game item drop chance and count tables\n",
    "- **reward_id_lot_table_mr**: sunbreak item drop chance and count tables\n",
    "\n",
    "OUTPUT: **df_drop_tables**\n",
    "| drop_table_id | lot_rule | item_id_list | num_list | probability_list |\n",
    "|-|-|-|-|-|\n",
    "| 310000 | Random | [125, 126, 687, 128, 699, 131, 719] | [1, 1, 1, 2, 1, 1, 1] | [17, 24, 15, 22, 8, 8, 6] |\n",
    "| 310001 | Random | [691, 132, 720, 705, 704, 129, 700] | [1, 1, 1, 1, 1, 1, 1] | [9, 14, 7, 19, 25, 14, 12] |\n",
    "| 310002 | Random | [693, 133, 710, 706, 707, 688, 701] | [1, 1, 1, 1, 1, 1, 1] | [5, 11, 16, 18, 14, 21, 15] |\n",
    "| 310003 | Random | [702, 484, 686, 689, 127, 133, 721] | [2, 1, 1, 1, 1, 1, 1] | [18, 17, 23, 14, 9, 13, 6] |\n",
    "| 310004 | Random | [695, 134, 479, 711, 690, 718, 127] | [1, 1, 1, 1, 1, 1, 1] | [5, 9, 19, 19, 14, 23, 11] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('breakdown/items.json') as file:\n",
    "    raw = json.load(file)\n",
    "\n",
    "# Item Drop Tables\n",
    "sect = raw.get('reward_id_lot_table').get('param')\n",
    "df = pd.json_normalize(sect)\n",
    "df['item_id_list'] = df['item_id_list'].apply(lambda y: [x[\"Normal\"] for x in y if isinstance(x,dict)])\n",
    "df['num_list'] = df['num_list'].apply(lambda y: [x for x in y if x>0])\n",
    "df['probability_list'] = df['probability_list'].apply(lambda y: [x for x in y if x>0])\n",
    "df_drop_tables = df.copy()\n",
    "\n",
    "# MR Item Drop Tables\n",
    "sect = raw.get('reward_id_lot_table_mr').get('param')\n",
    "df = pd.json_normalize(sect)\n",
    "df['item_id_list'] = df['item_id_list'].apply(lambda y: [x[\"Normal\"] for x in y if isinstance(x,dict)])\n",
    "df['num_list'] = df['num_list'].apply(lambda y: [x for x in y if x>0])\n",
    "df['probability_list'] = df['probability_list'].apply(lambda y: [x for x in y if x>0])\n",
    "df_drop_tables = pd.concat([df_drop_tables, df], axis=0, ignore_index=True)\n",
    "df_drop_tables.rename(columns={'id':'drop_table_id'}, inplace=True)\n",
    "\n",
    "# df_drop_tables.to_excel('item_drop_tables.xlsx')\n",
    "print(df_drop_tables.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Items, Subset of Items\n",
    "\n",
    "COLUMNS:\n",
    "- **item_id**: item ID\n",
    "- **item_name**: item name\n",
    "- **type**: material type [aff, ess, coin]\n",
    "- **category_worth**: item value in augmenting and charm melding\n",
    "\n",
    "OUTPUT: **df_anomaly_items**\n",
    "| item_id | item_name | type | category_worth |\n",
    "|-|-|-|-|\n",
    "| 2521 | Amber Essence | ess | 10 |\n",
    "| 2522 | Amber Essence+ | ess | 20 |\n",
    "| 2523 | Prime Amber Essence | ess | 40 |\n",
    "| 2584 | Afflicted Bone | aff | 2 |\n",
    "| 2585| Afflicted Pelt | aff | 2 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_items[~df_items['category_prefix'].str.contains('LrHr')] #remove LrHr items so that only Mr or None results show\n",
    "df_anomaly_items = df[df['material_category'].apply(lambda x: 73 in x)].copy()\n",
    "df_anomaly_items['type'] = 'aff'\n",
    "df_ess = df[df['material_category'].apply(lambda x: 74 in x)].copy()\n",
    "df_ess['type'] = 'ess'\n",
    "df_coin = df_items[df_items['item_name'].str.contains('Investigation Coin',case=False,na=False)].copy()\n",
    "df_coin['type'] = 'coin'\n",
    "df_anomaly_items = pd.concat([df_anomaly_items, df_ess, df_coin], axis=0, ignore_index=True)\n",
    "df_anomaly_items = df_anomaly_items[['item_id','item_name','type','category_worth']]\n",
    "df_anomaly_items.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# df_anomaly_items.to_excel('anom_items.xlsx')\n",
    "print(df_anomaly_items.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Drop Lots\n",
    "SECTIONS:\n",
    "- **mystery_reward_item**: anomaly drop lots for quests, investigations, and special investigations\n",
    "\n",
    "COLUMNS:\n",
    "- **lv_lower** (~~lv_lower_limit~~): Anomaly investigation reward bracket lower level. 0 for quests. 300 for special investigations.\n",
    "- **lv_upper** (~~lv_upper_limit~~): Anomaly investigation reward bracket upper level. 0 for quests. 300 for special investigations.\n",
    "- **em_type.Em**: primary target monster internal ID\n",
    "- **item_id** (~~reward_item.Normal~~): the primary afflicted material dropped by primary target monster on carve and part break\n",
    "- **anom_item_chance** (~~hagibui_probability~~): chance of getting primary afflicted material on carve and part break\n",
    "- **anom_drop_table_id** (~~quest_reward_table_index~~): table ID for quest rewards.\n",
    "- **anom_drop_lot_ids** (~~additional_quest_reward_table_index~~): additional quest reward table IDs.\n",
    "- **sp_ess_table_id** (~~special_quest_reward_table_index~~): bonus essence drop table for single AR brackets (e.g. AR200, AR220)\n",
    "- **multi_ess_table_id** (~~multiple_target_reward_table_index~~): bonus essence drop table for hunting monster as secondary target\n",
    "- **multi_coin_table_id**: (~~multiple_fix_reward_table_index~~): bonus coin drop table for hunting monster as secondary target\n",
    "- **mystery_reward_table**: ???\n",
    "\n",
    "OUTPUT: **df_anom_drop_lots**\n",
    "| lv_lower | lv_upper | anom_item_chance | anom_drop_table_id | anom_drop_lot_ids |\n",
    "|-|-|-|-|-|\n",
    "| 0 | 0 | 40 | 0 | [0, 0, 0, 0] |\n",
    "| 1 | 30 | 40 | 560000 | [550000, 540000, 0, 330300] |\n",
    "| 31 | 50 | 40 | 560000 | [550100, 540001, 550000, 330300] |\n",
    "| 51 | 70 | 40 | 560001 | [550100, 540100, 550000, 330300] |\n",
    "| 71 | 90 | 40 | 560001 | [550101, 540100, 550000, 330301] |\n",
    "\n",
    "| sp_ess_table_id | multi_ess_table_id | multi_coin_table_id | em_type.Em | item_id |\n",
    "|-|-|-|-|-|\n",
    "| 0 | 0 | 0 | 98 | 2585 |\n",
    "| 0 | 760000 | 770000 | 98 | 2585 |\n",
    "| 0 | 760000 | 770000 | 98 | 2885 |\n",
    "| 0 | 760001 | 770001 | 98 | 2885 |\n",
    "| 0 | 760001 | 770001 | 98 | 2885 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly Drop Lots\n",
    "sect = raw.get('mystery_reward_item').get('param')\n",
    "df = pd.json_normalize(sect)\n",
    "df = df[df['is_special_mystery']==False] # remove SI entries since drops are the same as AR300 and represented separately in [300,300] investigations\n",
    "df = df[~((df['lv_lower_limit']==0) & (df['lv_upper_limit']==0))] # remove 0-0 entries as they don't represent anomaly quest rewards\n",
    "\n",
    "df['reward_item.Normal'] = pd.to_numeric(df['reward_item.Normal'].fillna(-1), errors='ignore', downcast='integer')\n",
    "df.drop(['is_special_mystery','quest_no', 'item_num', 'reward_item', 'mystery_reward_table'], axis=1, inplace=True)\n",
    "\n",
    "df.rename(columns={'lv_lower_limit':'lv_lower',\n",
    "           'lv_upper_limit':'lv_upper',\n",
    "           'reward_item.Normal':'item_id',\n",
    "           'hagibui_probability':'anom_item_chance',\n",
    "           'quest_reward_table_index':'anom_drop_table_id',\n",
    "           'additional_quest_reward_table_index':'anom_drop_lot_ids',\n",
    "           'special_quest_reward_table_index':'sp_ess_table_id',\n",
    "           'multiple_target_reward_table_index':'multi_ess_table_id',\n",
    "           'multiple_fix_reward_table_index':'multi_coin_table_id'\n",
    "           },inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_anom_drop_lots = df.copy()\n",
    "print(df_anom_drop_lots.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Material Average Item Drop\n",
    "\n",
    "REFERENCES:\n",
    "- **df_anomaly_items**\n",
    "- **df_drop_tables**\n",
    "\n",
    "COLUMNS:\n",
    "- **drop_table_id**: drop table ID\n",
    "- **aff_avg**: average afflicted material melding worth\n",
    "- **ess_avg**: average essence augmenting worth\n",
    "- **coin_avg**: average investigation coin worth\n",
    "\n",
    "OUTPUT: **df_anom_drop_avg**\n",
    "| drop_table_id | aff_avg | ess_avg | coin_avg |\n",
    "|-|-|-|-|\n",
    "| 530000 | 4.8 | 0.0 | 0.0 |\n",
    "| 530001 | 4.8 | 0.0 | 0.0 |\n",
    "| 530002 | 9.6 | 0.0 | 0.0 |\n",
    "| 530003 | 9.6 | 0.0 | 0.0 |\n",
    "| 530004 | 14.4 | 0.0 | 0.0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out non-anomaly drop tables\n",
    "mapper = set(df_anomaly_items['item_id'])\n",
    "df = df_drop_tables[df_drop_tables['item_id_list'].apply(lambda x: any(y in mapper for y in x))]\n",
    "\n",
    "#add translated names to items\n",
    "# mapper = dict(zip(df_items['item_id'], df_items['item_name']))\n",
    "# df_drop_tables['tab']=df_drop_tables['item_id_list'].apply(lambda x: [mapper.get(i,'?') for i in x])\n",
    "\n",
    "# translate item_id_list into worth_list\n",
    "df2 = df_anomaly_items[df_anomaly_items['type']=='aff']\n",
    "mapper = dict(zip(df2['item_id'], df2['category_worth']))\n",
    "df['aff_worth_list'] = df['item_id_list'].apply(lambda x: [mapper.get(i,0) for i in x])\n",
    "df2 = df_anomaly_items[df_anomaly_items['type']=='ess']\n",
    "mapper = dict(zip(df2['item_id'], df2['category_worth']))\n",
    "df['ess_worth_list'] = df['item_id_list'].apply(lambda x: [mapper.get(i,0) for i in x])\n",
    "df2 = df_anomaly_items[df_anomaly_items['type']=='coin']\n",
    "mapper = dict(zip(df2['item_id'], [1]))\n",
    "df['coin_worth_list'] = df['item_id_list'].apply(lambda x: [mapper.get(i,0) for i in x])\n",
    "\n",
    "# sum of products: item_cts, item_chance, aff_worth\n",
    "df['aff_avg'] = df.apply(lambda x: round(np.sum(np.array(x['num_list']) * np.array(x['probability_list']) * np.array(x['aff_worth_list']) /100),2), axis=1)\n",
    "df['ess_avg'] = df.apply(lambda x: round(np.sum(np.array(x['num_list']) * np.array(x['probability_list']) * np.array(x['ess_worth_list']) /100),2), axis=1)\n",
    "df['coin_avg'] = df.apply(lambda x: round(np.sum(np.array(x['num_list']) * np.array(x['probability_list']) * np.array(x['coin_worth_list']) /100),2), axis=1)\n",
    "\n",
    "df_anom_drop_avg = df[['drop_table_id','aff_avg','ess_avg','coin_avg']].copy()\n",
    "\n",
    "# df_anom_drop_avg['am_avg'] = df_anom_drop_avg.apply(lambda x: [x['aff_avg'], x['ess_avg'], x['coin_avg']], axis=1)\n",
    "# df_anom_drop_avg = df[['drop_table_id','am_avg']]\n",
    "\n",
    "# df_anom_drop_avg.to_excel('anom_drop_avg.xlsx')\n",
    "print(df_anom_drop_avg.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Material Drop Per Anomaly Investigation Level Bracket\n",
    "REFERENCES:\n",
    "- **df_anom_drop_lots**\n",
    "- **df_anomaly_items**\n",
    "- **df_anom_drop_avg**\n",
    "- **df_monsters**\n",
    "\n",
    "COLUMNS:\n",
    "- **lv_lower**: anomaly investigation level bracket lower end\n",
    "- **lv_upper**: anomaly investigation level bracket upper end\n",
    "- **item_name**: primary afflicted material reward\n",
    "- **aff_avg**: average afflicted material augment and melding value\n",
    "- **ess_avg**: average essence augment value\n",
    "- **coin_avg**: average investigation coin value\n",
    "- **ess_avg_multi**: average essence augment value for multi-monster investigations\n",
    "- **coin_avg_multi**: average investigation coin value for multi-monster investigations\n",
    "- **mon_name**: primary target name\n",
    "- **mystery_rank**: primary target rank\n",
    "\n",
    "OUTPUT: **df_anom_avg**\n",
    "| lv_lower | lv_upper | item_name | aff_avg | ess_avg | coin_avg | ess_avg_multi | coin_avg_multi | mon_name | mystery_rank |\n",
    "|-|-|-|-|-|-|-|-|-|-|\n",
    "| 1 | 30 | Afflicted Pelt | 7.2 | 56.5 | 0.05 | 29 | 2 | Great Izuchi | 1 |\n",
    "| 31 | 50 | Afflicted Hide+ | 29.6 | 67.5 | 0.05 | 29 | 2 | Great Izuchi | 1 |\n",
    "| 51 | 70 | Afflicted Hide+ | 29.6 | 115 | 0.25 | 58 | 2.4 | Great Izuchi | 1 |\n",
    "| 71 | 90 | Afflicted Hide+ | 38.4 | 115 | 0.25 | 58 | 2.4 | Great Izuchi | 1 |\n",
    "| 91 | 100 | Afflicted Hide+ | 63.8 | 289 | 0.25 | 84 | 2.4 | Great Izuchi | 1 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carves = 4 # number of carves + estimated partbreaks\n",
    "df = df_anom_drop_lots\n",
    "\n",
    "#replace item_id with item name and worth from df_anomaly_items\n",
    "df = pd.merge(df, df_anomaly_items[['item_id','item_name','category_worth']], on='item_id', how='left')\n",
    "df.drop(columns=['item_id'], inplace=True)\n",
    "df.fillna(0,inplace=True)\n",
    "\n",
    "# #create carve reward worth column\n",
    "df['carve_worth'] = df.apply(lambda x: round(x['category_worth']*x['anom_item_chance']*carves/100,2), axis=1)\n",
    "df.drop(columns=['category_worth','anom_item_chance'], inplace=True)\n",
    "\n",
    "# #replace reward tables with am, ess, coin worth averages\n",
    "df = pd.merge(left=df, left_on='anom_drop_table_id', right=df_anom_drop_avg, right_on='drop_table_id', how='left')\n",
    "df.drop(columns=['anom_drop_table_id','drop_table_id'], inplace=True)\n",
    "df.fillna(0,inplace=True)\n",
    "# df = df.rename(columns={'am_avg':'quest_rewards'})\n",
    "df['aff_avg'] = df['aff_avg']+df['carve_worth']\n",
    "df.drop(columns='carve_worth', inplace=True)\n",
    "\n",
    "#replace additional quest rewards with am worth averages\n",
    "mapper = dict(zip(df_anom_drop_avg['drop_table_id'], list(zip(df_anom_drop_avg['aff_avg'],df_anom_drop_avg['ess_avg'],df_anom_drop_avg['coin_avg']))))\n",
    "df['aqr'] = df['anom_drop_lot_ids'].apply(lambda x: [mapper.get(i,[0,0,0]) for i in x])\n",
    "#zip addition quest rewards\n",
    "df['aqr_sum'] = df['aqr'].apply(lambda x: [round(sum(i),2) for i in zip(*x)])\n",
    "df.drop(columns=['aqr','anom_drop_lot_ids'], inplace=True)\n",
    "#individual column sum\n",
    "df['aff_avg'] = df.apply(lambda x: x['aff_avg']+x['aqr_sum'][0], axis=1)\n",
    "df['ess_avg'] = df.apply(lambda x: x['ess_avg']+x['aqr_sum'][1], axis=1)\n",
    "df['coin_avg'] = df.apply(lambda x: x['coin_avg']+x['aqr_sum'][2], axis=1)\n",
    "df.drop(columns='aqr_sum', inplace=True)\n",
    "\n",
    "#sp essence rewards\n",
    "df = pd.merge(left=df, left_on='sp_ess_table_id', right=df_anom_drop_avg[['drop_table_id','ess_avg']], right_on='drop_table_id', how='left', suffixes=['','_sp'])\n",
    "df.fillna(0,inplace=True)\n",
    "df['ess_avg'] = df['ess_avg']+df['ess_avg_sp']\n",
    "df.drop(columns=['drop_table_id','ess_avg_sp','sp_ess_table_id'], inplace=True)\n",
    "\n",
    "#multi rewards\n",
    "df = pd.merge(left=df, left_on='multi_ess_table_id', right=df_anom_drop_avg[['drop_table_id','ess_avg']], right_on='drop_table_id', how='left', suffixes=['','_multi'])\n",
    "df.fillna(0,inplace=True)\n",
    "df.drop(columns=['multi_ess_table_id','drop_table_id'], inplace=True)\n",
    "df = pd.merge(left=df, left_on='multi_coin_table_id', right=df_anom_drop_avg[['drop_table_id','coin_avg']], right_on='drop_table_id', how='left', suffixes=['','_multi'])\n",
    "df.fillna(0,inplace=True)\n",
    "df.drop(columns=['multi_coin_table_id','drop_table_id'], inplace=True)\n",
    "\n",
    "#monster names\n",
    "df = pd.merge(df, df_monsters[['em_type.Em','mon_name','mystery_rank']], on='em_type.Em',how='left')\n",
    "df.drop(columns=['em_type.Em'], inplace=True)\n",
    "\n",
    "df_anom_avg = df.copy()\n",
    "# df_anom_avg.to_excel('anom_avg.xlsx')\n",
    "print(df_anom_avg.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quest Names & IDs\n",
    "\n",
    "SECTIONS:\n",
    "- **quest_arena_msg**\n",
    "- **quest_dlc_msg**\n",
    "- **quest_hall_msg**\n",
    "- **quest_hall_msg_mr**\n",
    "- **quest_hall_msg_mr2**\n",
    "- **quest_tutorial_msg**\n",
    "- **quest_village_msg**\n",
    "- **npc_mission_msg**\n",
    "- **npc_mission_msg_mr**\n",
    "\n",
    "COLUMNS:\n",
    "- **quest_id**: internal quest ID\n",
    "- **quest_type**: quest category\n",
    "- **quest_name**: quest name\n",
    "\n",
    "OUTPUT: **df_quest_names**\n",
    "| quest_id | quest_name |\n",
    "|-|-|\n",
    "| 20101 | Arena 01 |\n",
    "| 20201 | Arena 02 |\n",
    "| 20301 | Arena 03 |\n",
    "| 20601 | Arena 04 |\n",
    "| 20701 | Arena 05 |\n",
    "\n",
    "OUTPUT2: **df_npc_quest_names**\n",
    "| quest_id | quest_name |\n",
    "|-|-|\n",
    "| 1 | Elder Fugen's Errand |\n",
    "| 2 | Thick, Gooey Dango |\n",
    "| 3 | Bunny Dango—with Eggs! |\n",
    "| 4 | A Dango a Day |\n",
    "| 5 | Only the Good Eggs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     quest_id                         quest_name\n",
      "0       20101                           Arena 01\n",
      "1       20201                           Arena 02\n",
      "2       20301                           Arena 03\n",
      "3       20601                           Arena 04\n",
      "4       20701                           Arena 05\n",
      "..        ...                                ...\n",
      "698       605                   Thundering Voice\n",
      "699       606                     Twisted Desire\n",
      "700       607                  A Test of Courage\n",
      "701       608          Like a Flash of Lightning\n",
      "702       609  King of the Sky, Bane of the Land\n",
      "\n",
      "[703 rows x 2 columns]\n",
      "    quest_id                     quest_name\n",
      "0          1           Elder Fugen's Errand\n",
      "1          2             Thick, Gooey Dango\n",
      "2          3         Bunny Dango—with Eggs!\n",
      "3          4                  A Dango a Day\n",
      "4          5             Only the Good Eggs\n",
      "..       ...                            ...\n",
      "80       146        Resplendent Red Request\n",
      "81       148                  Bag a Barioth\n",
      "82       149  A Great Gift for Your Buddies\n",
      "83       151    Giant in the Frost Islands?\n",
      "84       152     In Cahoots for Cohoot Pics\n",
      "\n",
      "[85 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "with open('breakdown/quests.json') as file:\n",
    "    raw = json.load(file)\n",
    "\n",
    "# Quest Lists\n",
    "sections = ['quest_arena_msg',\n",
    "           'quest_dlc_msg',\n",
    "           'quest_hall_msg',\n",
    "           'quest_hall_msg_mr',\n",
    "           'quest_hall_msg_mr2',\n",
    "           'quest_tutorial_msg',\n",
    "           'quest_village_msg',\n",
    "           'npc_mission_msg',\n",
    "           'npc_mission_msg_mr']\n",
    "df_quest_names = pd.DataFrame()\n",
    "for q in sections:\n",
    "    sect = raw.get(q).get('entries')\n",
    "    df = pd.json_normalize(sect)\n",
    "    df = df[df['name'].str.contains('_01')]\n",
    "    df['quest_type'] = df['name'].str.extract('(^[A-Za-z]+)')\n",
    "    df['quest_id'] = df['name'].str.extract('(\\d+)')\n",
    "    df['quest_id'] = pd.to_numeric(df['quest_id'],errors='coerce',downcast='integer')\n",
    "    df['content'] = df['content'].apply(lambda x: x[1])\n",
    "    df = df[~df['content'].str.contains('COLOR FF0000', na=False)]\n",
    "    df_quest_names = pd.concat([df_quest_names,df[['quest_id','quest_type','content']]], axis=0,ignore_index=True)\n",
    "\n",
    "df_quest_names.rename(columns={'content':'quest_name'}, inplace=True)\n",
    "\n",
    "df_npc_quest_names = df_quest_names[df_quest_names['quest_type']=='NSQ'].copy()\n",
    "df_npc_quest_names = df_npc_quest_names[['quest_id','quest_name']]\n",
    "df_npc_quest_names.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_quest_names = df_quest_names[df_quest_names['quest_type']=='QN']\n",
    "df_quest_names = df_quest_names[['quest_id','quest_name']]\n",
    "df_quest_names.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(df_quest_names)\n",
    "print(df_npc_quest_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quest Levels & Targets\n",
    "\n",
    "COLUMNS:\n",
    "- target_type:\n",
    "    - Kill: slay target\n",
    "    - AllMainEnemy: hunt all listed under secondary\n",
    "    - Capture: capture target\n",
    "    - ItemGet: collect item\n",
    "    - Hunting: slay or capture\n",
    "    - EmTotal: slay small monsters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('breakdown/quests.json') as file:\n",
    "    raw = json.load(file)\n",
    "\n",
    "# Quest Levels\n",
    "sections = ['normal_quest_data',\n",
    "            'normal_quest_data_mr',\n",
    "            'dl_quest_data',\n",
    "            'dl_quest_data_mr']\n",
    "df = pd.DataFrame()\n",
    "for x in sections:\n",
    "    sect = raw.get(x).get('param')\n",
    "    df = pd.concat([df,pd.json_normalize(sect)], axis=0, ignore_index=True)\n",
    "df = df[['quest_no',\n",
    "         'quest_level',\n",
    "         'enemy_level',\n",
    "         'order_type',\n",
    "         'quest_type',\n",
    "         'target_type',\n",
    "         'tgt_em_type',\n",
    "         'boss_em_type',\n",
    "         'auto_match_hr']].reset_index(drop=True)\n",
    "#large monsters only\n",
    "df['tgt_em_type'] = df['tgt_em_type'].apply(lambda x: [i['Em'] for i in x if 'Em' in i])\n",
    "df['tgt_em_type'] = df['tgt_em_type'].apply(lambda x: [i for i in x if i>0])\n",
    "df['boss_em_type'] = df['boss_em_type'].apply(lambda x: [i['Em'] for i in x])\n",
    "df['boss_em_type'] = df['boss_em_type'].apply(lambda x: [i for i in x if i>0])\n",
    "\n",
    "mapper = dict(zip(df_monsters['em_type.Em'], df_monsters['mon_name']))\n",
    "df['tgt_em_type'] = df['tgt_em_type'].apply(lambda x: [mapper.get(i,0) for i in x])\n",
    "df['boss_em_type'] = df['boss_em_type'].apply(lambda x: [mapper.get(i,0) for i in x])\n",
    "\n",
    "df = pd.merge(left=df, left_on='quest_no',\n",
    "              right=df_quest_names, right_on='quest_id',\n",
    "              how='left')\n",
    "\n",
    "'''\n",
    "Consolidate primary and 2ndary target columns\n",
    "'''\n",
    "\n",
    "df['tgt_em_type'] = df.apply(lambda x: x['boss_em_type'] if 'AllMainEnemy' in x['target_type'] else x['tgt_em_type'], axis=1)\n",
    "\n",
    "# df.rename(columns={'tgt_em_type':'tgt_em_type.Em',\n",
    "#                    'boss_em_type':'snd_em_type.Em',\n",
    "#                    'quest_no':'quest_id'}, inplace=True)\n",
    "\n",
    "\n",
    "# print(df_quests.iloc[604:606])\n",
    "# df_quests.to_excel('quest.xlsx')\n",
    "# df.to_excel('raw.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   quest_no  random_seed                               attr  \\\n",
      "0       304            0                   [FIX_WAVE_ORDER]   \n",
      "1     10216            0                                 []   \n",
      "2     10403            0                                 []   \n",
      "3     10603            0                                 []   \n",
      "4     60109            0                                 []   \n",
      "5     60113            0                                 []   \n",
      "6     60116            0  [FIX_WAVE_ORDER, FINAL_BOSS_KILL]   \n",
      "7     60153            0                                 []   \n",
      "8     60139            0                                 []   \n",
      "9     60144            0                   [FIX_WAVE_ORDER]   \n",
      "\n",
      "                                           wave_data quest_lv  map_no  \\\n",
      "0  [{'boss_em': {'Em': 60}, 'boss_sub_type': 0, '...      QL3       7   \n",
      "1  [{'boss_em': {'Em': 92}, 'boss_sub_type': 0, '...      QL2       7   \n",
      "2  [{'boss_em': {'Em': 98}, 'boss_sub_type': 0, '...      QL4       7   \n",
      "3  [{'boss_em': {'Em': 98}, 'boss_sub_type': 0, '...      QL6       7   \n",
      "4  [{'boss_em': {'Em': 90}, 'boss_sub_type': 0, '...      QL7       7   \n",
      "5  [{'boss_em': {'Em': 59}, 'boss_sub_type': 0, '...    QL7Ex       7   \n",
      "6  [{'boss_em': {'Em': 1}, 'boss_sub_type': 0, 'o...    QL7Ex       7   \n",
      "7  [{'boss_em': {'Em': 60}, 'boss_sub_type': 0, '...    QL7Ex       7   \n",
      "8  [{'boss_em': {'Em': 2}, 'boss_sub_type': 0, 'o...    QL7Ex       7   \n",
      "9  [{'boss_em': {'Em': 23}, 'boss_sub_type': 0, '...    QL7Ex       7   \n",
      "\n",
      "  category  is_village  base_time  start_block_no  end_block_no  \\\n",
      "0   Normal        True         13               3             3   \n",
      "1   Normal       False         13               1             1   \n",
      "2    Nushi       False          9               3             4   \n",
      "3    Nushi       False          0               3             4   \n",
      "4   Normal       False          9               1             1   \n",
      "5    Nushi       False          9               3             4   \n",
      "6    Nushi       False          0               3             4   \n",
      "7    Nushi       False          0               3             4   \n",
      "8   Normal       False          9               2             2   \n",
      "9   Normal       False          9               1             1   \n",
      "\n",
      "   extra_em_wave_no  extra_em_nando_tbl_no  nushi_order_tbl_no  \\\n",
      "0                 0                     -1                   0   \n",
      "1                 0                     -1                   0   \n",
      "2                 0                     -1                   0   \n",
      "3                 0                     -1                   0   \n",
      "4                 0                     -1                   0   \n",
      "5                 0                     -1                   0   \n",
      "6                 2                     -1                   0   \n",
      "7                 2                     -1                   0   \n",
      "8                 0                     -1                   0   \n",
      "9                 0                     -1                   0   \n",
      "\n",
      "   hm_unlock_tbl_no                                         sub_target  \\\n",
      "0                 0               [None, None, None, None, None, None]   \n",
      "1                 1  [EmStun, EmCntHmBallista, EmCntHmNpc, EmCntHmC...   \n",
      "2                 0  [EmCondition, EmCntHmCannon, DropItem, Hunting...   \n",
      "3                 0  [EmCntHmBallista, EmCntWeapon, DropItem, EmEle...   \n",
      "4                 1  [HuntingMachine, EmStun, EmCntHmBallista, EmCn...   \n",
      "5                 0  [HuntingMachine, EmCntHmBallista, DropItem, Em...   \n",
      "6                 0  [EmCntHmBallista, EmCntWeapon, EmStun, DropIte...   \n",
      "7                 0  [EmCntHmBallista, EmCntWeapon, EmStun, DropIte...   \n",
      "8                 0  [HuntingMachine, EmCntHmBallista, DropItem, Em...   \n",
      "9                 0  [HuntingMachine, EmCntHmBallista, DropItem, Em...   \n",
      "\n",
      "   sub_target5_wave_no  \n",
      "0                    0  \n",
      "1                    0  \n",
      "2                    2  \n",
      "3                    1  \n",
      "4                    0  \n",
      "5                    0  \n",
      "6                    2  \n",
      "7                    2  \n",
      "8                    0  \n",
      "9                    0  \n"
     ]
    }
   ],
   "source": [
    "'''fill this one out for above cell'''\n",
    "\n",
    "# Rampage details\n",
    "sect = raw.get('fixed_hyakuryu_quest')\n",
    "sections = pd.json_normalize(sect).columns.tolist()\n",
    "df = pd.DataFrame()\n",
    "for x in sections:\n",
    "    df = pd.concat([df,pd.json_normalize(sect.get(x))], axis=0, ignore_index=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quest Unlock by Talk Flag & Misc\n",
    "sect = raw.get('quest_unlock').get('quest_unlock_by_talk_flag')\n",
    "df_quest_talk_flag = pd.json_normalize(sect)\n",
    "df_quest_talk_flag.drop(columns='is_clear', inplace=True)\n",
    "df_quest_talk_flag.rename(columns={'quest_no':'quest_id'}, inplace=True)\n",
    "\n",
    "print(df_quest_talk_flag.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quest Unlock by Quest Clear\n",
    "sect = raw.get('quest_unlock').get('quest_unlock_by_quest_clear')\n",
    "df_quest_clear_flag = pd.json_normalize(sect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quest Unlock by Monster Hunt\n",
    "sect = raw.get('quest_unlock').get('quest_unlock_by_hunt_enemy')\n",
    "df_quest_hunt_flag = pd.json_normalize(sect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## progress flags\n",
    "OUTPUT: **df_prog_flags**\n",
    "| progress_flag | quest_name | rank |\n",
    "|-|-|-|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress Flags\n",
    "sect = raw.get('progress').get('param_list')\n",
    "df = pd.json_normalize(sect)\n",
    "df.fillna(-1,inplace=True)\n",
    "# df.rename(columns={'progress_flag':'',\n",
    "#                    'quest_no':'',\n",
    "#                    'enable_progress_hr_check':'',\n",
    "#                    'progress_hr':'',\n",
    "#                    'village.VillageProgress':'',\n",
    "#                    'hall.HallProgress':'',\n",
    "#                    'mr.MasterRankProgress':''}, inplace=True)\n",
    "df = pd.merge(left=df, left_on='quest_no', right=df_quest_names[['quest_id','quest_name']], right_on='quest_id', how='left')\n",
    "df['village.VillageProgress'] = pd.to_numeric(df['village.VillageProgress'],errors='coerce', downcast='integer')\n",
    "df['hall.HallProgress'] = pd.to_numeric(df['hall.HallProgress'],errors='coerce', downcast='integer')\n",
    "df['mr.MasterRankProgress'] = pd.to_numeric(df['mr.MasterRankProgress'],errors='coerce', downcast='integer')\n",
    "df['rank'] = df.apply(lambda x: [x['village.VillageProgress'],x['hall.HallProgress'],x['mr.MasterRankProgress']], axis=1)\n",
    "# df_prog_flags = df[['progress_flag','quest_no','quest_name','enable_progress_hr_check','progress_hr','rank']].copy()\n",
    "df_prog_flags = df[['progress_flag','talk_flag','quest_name','progress_hr','rank']].copy()\n",
    "\n",
    "#talk flags\n",
    "df_prog_flags = pd.merge(left=df_prog_flags, left_on='talk_flag', right=df_quest_talk_flag, right_on='talk_flag', how='left')\n",
    "df_prog_flags.drop(columns='talk_flag', inplace=True)\n",
    "# df_prog_flags.rename(columns={'quest_name':'quest_name_1'}, inplace=True)\n",
    "df_prog_flags = pd.merge(left=df_prog_flags, left_on='quest_id', right=df_quest_names[['quest_id','quest_name']], right_on='quest_id', how='left')\n",
    "df_prog_flags.drop(columns='quest_id', inplace=True)\n",
    "\n",
    "df_prog_flags ['quest_name'] = df_prog_flags['quest_name_x'].fillna(df_prog_flags['quest_name_y'])\n",
    "df_prog_flags = df_prog_flags[['progress_flag','quest_name','rank']]\n",
    "\n",
    "print(df_prog_flags.head())\n",
    "# df_prog_flags.to_excel('prog_flags.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skills\n",
    "\n",
    "## Skill Name & ID\n",
    "\n",
    "SECTIONS:\n",
    "- **player_skill_name_msg**\n",
    "- **player_skill_name_msg_mr**\n",
    "\n",
    "COLUMNS:\n",
    "- **skill_id**: armor skill internal id. HR: 0-199, MR: 200-235, Rampage MR: 259-293\n",
    "- **skill_name**: armor skill name\n",
    "\n",
    "OUTPUT: **df_skill_names**\n",
    "| skill_id | skill_name |\n",
    "|-|-|\n",
    "| 0 | Attack Boost |\n",
    "| 1 | Agitator |\n",
    "| 2 | Peak Performance |\n",
    "| 3 | Resentment |\n",
    "| 4 | Resuscitate |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('breakdown/skills.json') as file:\n",
    "    raw = json.load(file)\n",
    "\n",
    "# Skill Names & IDs\n",
    "sections = ['player_skill_name_msg',\n",
    "            'player_skill_name_msg_mr',\n",
    "            # 'hyakuryu_skill_name_msg',\n",
    "            'hyakuryu_skill_name_msg_mr']\n",
    "df_skill_names = pd.DataFrame()\n",
    "for x in sections:\n",
    "    sect = raw.get(x).get('entries')\n",
    "    df = pd.json_normalize(sect)\n",
    "    df['skill_name'] = df['content'].apply(lambda x: x[1]) #grab only english name\n",
    "    df['skill_id'] = df['name'].str.extract('(\\d+)', expand=False) # grab id numbers\n",
    "    df = df[df['skill_name']!=''] # remove blank names\n",
    "    df.dropna(inplace=True) # remove blank ids\n",
    "    df = df[~df['skill_name'].str.contains('COLOR FF0000', na=False)]\n",
    "    df['skill_id'] = pd.to_numeric(df['skill_id'], errors='ignore', downcast='integer')\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df_skill_names = pd.concat([df_skill_names,df[['skill_id', 'skill_name']]], axis=0, ignore_index=True)\n",
    "df_skill_names.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(df_skill_names.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Armor Names & IDs\n",
    "\n",
    "OUTPUT: df_armors\n",
    "| series_id | armor_type | armor_name | series_name |\n",
    "|-|-|-|-|\n",
    "| 0 | Head | Kamura Head Scarf | Kamura |\n",
    "| 1 | Head | Kamura Head Scarf S | Kamura S |\n",
    "| 2 | Head | Leather Headgear | Leather |\n",
    "| 3 | Head | Leather Headgear S | Leather S |\n",
    "| 4 | Head | Chainmail Headgear | Chainmail |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('breakdown/armors.json') as file:\n",
    "    raw = json.load(file)\n",
    "\n",
    "# Armor Names & IDs\n",
    "sections = [\n",
    "    'armor_head_name_msg',\n",
    "    'armor_chest_name_msg',\n",
    "    'armor_arm_name_msg',\n",
    "    'armor_waist_name_msg',\n",
    "    'armor_leg_name_msg',\n",
    "    'armor_head_name_msg_mr',\n",
    "    'armor_chest_name_msg_mr',\n",
    "    'armor_arm_name_msg_mr',\n",
    "    'armor_waist_name_msg_mr',\n",
    "    'armor_leg_name_msg_mr'\n",
    "]\n",
    "df_armor_names = pd.DataFrame()\n",
    "for x in sections:\n",
    "    sect = raw.get(x).get('entries')\n",
    "    df = pd.json_normalize(sect)\n",
    "    df_armor_names = pd.concat([df_armor_names,df],axis=0,ignore_index=True)\n",
    "\n",
    "\n",
    "df_armor_names['armor_name'] = df_armor_names['content'].apply(lambda x: x[1] if not x[1]=='' else np.nan)\n",
    "df_armor_names = df_armor_names[~df_armor_names['armor_name'].str.contains('COLOR FF0000', na=False)]\n",
    "df_armor_names.dropna(subset=['armor_name'], inplace=True)\n",
    "df_armor_names['series_id'] = df_armor_names['name'].str.extract('(\\d+)', expand=False)\n",
    "df_armor_names['series_id'] = pd.to_numeric(df_armor_names['series_id'], errors='coerce', downcast='integer')\n",
    "df_armor_names['armor_type'] = df_armor_names['name'].str.extract('A_([^_]+)', expand=False)\n",
    "\n",
    "df_armor_names = df_armor_names[['series_id','armor_type','armor_name']]\n",
    "df_armor_names.reset_index(drop=True, inplace=True)\n",
    "\n",
    "sections = ['armor_series_name_msg','armor_series_name_msg_mr']\n",
    "df_armor_series = pd.DataFrame()\n",
    "for x in sections:\n",
    "    sect = raw.get(x).get('entries')\n",
    "    df = pd.json_normalize(sect)\n",
    "    df_armor_series = pd.concat([df_armor_series,df],axis=0,ignore_index=True)\n",
    "\n",
    "df_armor_series['series_id'] = df_armor_series['name'].str.extract('(\\d+)', expand=False)\n",
    "df_armor_series['series_name'] = df_armor_series['content'].apply(lambda x: x[1] if not x[1]=='' else np.nan)\n",
    "df_armor_series = df_armor_series[~df_armor_series['series_name'].str.contains('COLOR FF0000', na=False)]\n",
    "df_armor_series.dropna(subset=['series_name'], inplace=True)\n",
    "df_armor_series['series_id'] = pd.to_numeric(df_armor_series['series_id'], errors='coerce', downcast='integer')\n",
    "\n",
    "df_armor_series = df_armor_series[['series_id','series_name']]\n",
    "df_armor_series.reset_index(drop=True,inplace=True)\n",
    "\n",
    "df_armors = pd.merge(left=df_armor_names, left_on='series_id',\n",
    "                     right=df_armor_series,right_on='series_id',how='left')\n",
    "\n",
    "print(df_armors.head())\n",
    "# df_armor_names.to_excel('armor_names.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender Specific Armor Name Pairs\n",
    "\n",
    "SECTIONS:\n",
    "- **armor_pair**\n",
    "\n",
    "COLUMNS:\n",
    "- **male_armor_id**\n",
    "- **female_armor_id**\n",
    "- **male_armor_name**\n",
    "- **female_armor_name**\n",
    "\n",
    "OUTPUT: **df_armor_pair**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armor Pair\n",
    "sect = raw.get('armor_pair').get('param')\n",
    "df = pd.json_normalize(sect)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armor Unlock Conditions\n",
    "\n",
    "OUTPUT: **df_armor_flags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armor Unlock Conditions\n",
    "sect = raw.get('armor_product').get('param')\n",
    "df = pd.json_normalize(sect)\n",
    "\n",
    "sections = ['id.Head', 'id.Chest', 'id.Arm', 'id.Waist', 'id.Leg']\n",
    "df['series_id']=np.nan\n",
    "for y in sections:\n",
    "    df['series_id'].fillna(df[y], inplace=True)\n",
    "df['series_id'] = pd.to_numeric(df['series_id'], errors='coerce',downcast='integer')\n",
    "df['armor_type']=np.nan\n",
    "for y in sections:\n",
    "    df['armor_type'] = df.apply(lambda x: y[3:] if pd.notna(x[y]) else x['armor_type'], axis=1)\n",
    "df['enemy_flag.Em'].fillna(df['enemy_flag.Ems'], inplace=True)\n",
    "df['item'] = df['item'].apply(lambda y: [x[\"Normal\"] for x in y if isinstance(x,dict)])\n",
    "df['item_num'] = df['item_num'].apply(lambda y: [x for x in y if x>0])\n",
    "\n",
    "df = pd.merge(left=df, left_on=['series_id', 'armor_type'],\n",
    "              right=df_armors, right_on=['series_id', 'armor_type'],\n",
    "              how='left')\n",
    "\n",
    "df.fillna(-1, inplace=True)\n",
    "df['enemy_flag.Em'] = pd.to_numeric(df['enemy_flag.Em'], errors='coerce', downcast='integer')\n",
    "df['item_flag.Normal'] = pd.to_numeric(df['item_flag.Normal'], errors='coerce', downcast='integer')\n",
    "\n",
    "df.rename(columns={'item':'craft_item_id_list',\n",
    "                   'item_num':'craft_num_list',\n",
    "                   'enemy_flag.Em':'key_em_type.Em',\n",
    "                   'item_flag.Normal':'key_item_id'}, inplace=True)\n",
    "df_armor_flags = df[['series_name','armor_type','armor_name','craft_item_id_list','craft_num_list','progress_flag','key_em_type.Em','key_item_id']].copy()\n",
    "# df_armor_flags = df\n",
    "df_armor_flags.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(df_armor_flags.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human readable armor flags\n",
    "df_armor_flags = pd.merge(left=df_armor_flags, left_on='progress_flag',\n",
    "                          right=df_prog_flags, right_on='progress_flag',\n",
    "                          how='left')\n",
    "df_armor_flags.drop(columns='progress_flag',axis=1, inplace=True)\n",
    "\n",
    "df_armor_flags = pd.merge(left=df_armor_flags, left_on='key_em_type.Em',\n",
    "                          right=df_monsters[['em_type.Em','mon_name']], right_on='em_type.Em',\n",
    "                          how='left')\n",
    "df_armor_flags.drop(columns=['key_em_type.Em','em_type.Em'], axis=1, inplace=True)\n",
    "\n",
    "df_armor_flags = pd.merge(left=df_armor_flags, left_on='key_item_id',\n",
    "                          right=df_items[['item_id','item_name']], right_on='item_id',\n",
    "                          how='left')\n",
    "df_armor_flags.drop(columns=['key_item_id', 'item_id'], axis=1, inplace=True)\n",
    "\n",
    "print(df_armor_flags.head())\n",
    "# df_armor_flags.to_excel('armor_flags.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decorations\n",
    "\n",
    "## Schema\n",
    "\n",
    "### \"decorations\": param\n",
    "- **decorations**: \"id\":{\"Deco\": 11}\n",
    "- **decoration_lv**: 2\n",
    "- **skill_id_list**: [{\"Skill\": 11},\"None\"]\n",
    "- **skill_lv_list**: [1,0]\n",
    "\n",
    "### \"decorations_product\": param\n",
    "- **id**: {\"Deco\": 0}\n",
    "- **item_flag**: \"None\"\n",
    "- **enemy_flag**: {\"Em\": 99}\n",
    "- **progress_flag**: 0\n",
    "- **item_id_list**: [{\"Normal\": 1036},\n",
    "                     {\"Normal\": 337},\n",
    "                     {\"Normal\": 571},\n",
    "                     {\"Normal\": 179}]\n",
    "- **item_num_list**: [5,3,3,1]\n",
    "\n",
    "### \"decorations_name_msg\":entries\n",
    "- **name**: \"Decorations_000_Name\n",
    "- **content**: [\"...\", \"Attack Jewel 2\", \"...\"]\n",
    "\n",
    "### \"decorations_name_msg_mr\":entries\n",
    "- **name**: \"Decorations_201_Name\n",
    "- **content**: [\"...\", \"Hard Fire Res Jewel 4\", \"...\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deco Names, Skills, & IDs\n",
    "REFERENCES:\n",
    "- **df_skill_names**\n",
    "\n",
    "SECTIONS:\n",
    "- **decoration_name_msg**\n",
    "- **decorations_name_msg_mr**\n",
    "- **decorations**\n",
    "\n",
    "COLUMNS:\n",
    "- **deco_id**: internal deco id number. HR: 0-199, MR: 200-352, Rampage: 400-434\n",
    "- **deco_name**: decoration name\n",
    "- **deco_size**: decoration size\n",
    "- **skill_name**: name of skill that decoration provides\n",
    "\n",
    "OUTPUT: **df_decos**\n",
    "| deco_id | deco_name | deco_size | skill_name |\n",
    "|-|-|-|-|\n",
    "| 0 | Attack Jewel 2 | 2 | Attack Boost |\n",
    "| 1 | Challenger Jewel 2 | 2 | Agitator |\n",
    "| 2 | Flawless Jewel 2 | 2 | Peak Performance |\n",
    "| 3 | Furor Jewel 2 | 2 | Resentment |\n",
    "| 4 | Crisis Jewel 2 | 2 | Resuscitate |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('breakdown/decos.json') as file:\n",
    "    raw = json.load(file)\n",
    "\n",
    "# Decoration Names\n",
    "deco_name_list = ['decorations_name_msg','decorations_name_msg_mr','hyakuryu_decos_name_msg']\n",
    "df_deco_names = pd.DataFrame()\n",
    "for x in deco_name_list:\n",
    "    sect = raw.get(x).get('entries')\n",
    "    df = pd.json_normalize(sect)\n",
    "    df['deco_name'] = df['content'].apply(lambda x: x[1]) #grab only english name\n",
    "    df['deco_id'] = df['name'].str.extract('(\\d+)', expand=False) # grab id numbers\n",
    "    df.dropna(inplace=True) # remove blank ids\n",
    "    df['deco_id'] = pd.to_numeric(df['deco_id'], errors='ignore', downcast='integer')\n",
    "    if x == 'hyakuryu_decos_name_msg':\n",
    "        df['deco_id'] = df['deco_id']+400\n",
    "    df = df[df['deco_name']!=''] # remove blank names\n",
    "    df = df[~df['deco_name'].str.contains('COLOR FF0000', na=False)]\n",
    "    df_deco_names = pd.concat([df_deco_names, df[['deco_id', 'deco_name']]], axis=0, ignore_index=True)\n",
    "df_deco_names.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# print(df_deco_names)\n",
    "\n",
    "# Decoration Skills\n",
    "sect = raw.get('decorations').get('param')\n",
    "df = pd.json_normalize(sect)\n",
    "df['skill_grade'] = df['skill_id_list'].apply(lambda x: next(iter(x[0]))) # grab MR category\n",
    "df['skill_id'] = df['skill_id_list'].apply(lambda x: next(iter(x[0].values()))) # grab skill ID\n",
    "df.loc[df['skill_grade']=='MrSkill', 'skill_id'] += 200 # convert skill ID to +200 for MR\n",
    "df['id.Deco'].fillna(df['id.MrDeco']+200, inplace=True) # merge MrDeco ID into Deco IDs\n",
    "df.rename(columns={'id.Deco':'deco_id',\n",
    "                'decoration_lv':'deco_size'}, inplace=True)\n",
    "df_deco_skills = df[['deco_id','deco_size','skill_id']].copy()\n",
    "\n",
    "#Rampage Decoration Skills\n",
    "sect = raw.get('hyakuryu_decos').get('param')\n",
    "df = pd.json_normalize(sect)\n",
    "df['id.Deco'] = df['id.Deco']+400\n",
    "df.rename(columns={'id.Deco':'deco_id',\n",
    "                'decoration_lv':'deco_size',\n",
    "                'hyakuryu_skill_id.Skill':'skill_id'}, inplace=True)\n",
    "df_deco_skills = pd.concat([df_deco_skills, df[['deco_id','deco_size','skill_id']]], axis=0, ignore_index=True)\n",
    "df_deco_skills['deco_id'] = pd.to_numeric(df_deco_skills['deco_id'], errors='coerce', downcast='integer')\n",
    "df_deco_skills.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# print(df_deco_skills)\n",
    "\n",
    "#\n",
    "df_decos = pd.DataFrame()\n",
    "# df_deco_skills = pd.merge(left=df[['deco_id','deco_size','skill_id']], left_on='skill_id', right=df_skill_names, right_on='skill_id', how='left')\n",
    "df_decos = pd.merge(left=df_deco_skills, left_on='skill_id', right=df_skill_names, right_on='skill_id', how='left')\n",
    "df_decos.drop(columns=['skill_id'], inplace=True)\n",
    "\n",
    "df_decos = pd.merge(left=df_deco_names, left_on='deco_id', right=df_decos, right_on='deco_id', how='left')\n",
    "df_decos.dropna(axis=0, inplace=True)\n",
    "df_decos['deco_size'] = pd.to_numeric(df_decos['deco_size'], errors='coerce',downcast='integer')\n",
    "# print(df_deco_skills)\n",
    "\n",
    "print(df_decos.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deco Unlock Conditions\n",
    "\n",
    "SECTIONS:\n",
    "- **decorations_product**\n",
    "\n",
    "COLUMNS:\n",
    "- **\n",
    "\n",
    "OUTPUT:\n",
    "| deco_id | craft_item_id_list | craft_num_list | progress_flag | key_em_type.Em | key_item_id |\n",
    "|-|-|-|-|-|-|\n",
    "| 0 | [1036, 337, 571, 179] | [5, 3, 3, 1] | 0 | 99 | -1 |\n",
    "| 1 | [1037, 645, 560, 904] | [4, 2, 3, 1] | 3 | 0 | -1 |   \n",
    "| 2 | [1037, 604, 581, 281] | [4, 2, 3, 1] | 3 | 1874 | -1 |  \n",
    "| 3 | [1037, 330, 266, 578] | [4, 2, 3, 2] | 3 | 0 | -1 |\n",
    "| 4 | [1037, 285, 327, 904] | [4, 3, 2, 1] | 3 | 0 | -1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoration Unlock Conditions\n",
    "\n",
    "sect = raw.get('decorations_product').get('param')\n",
    "df = pd.json_normalize(sect)\n",
    "df['item_id_list'] = df['item_id_list'].apply(lambda y: [x[\"Normal\"] for x in y if isinstance(x,dict)])\n",
    "df['item_num_list'] = df['item_num_list'].apply(lambda y: [x for x in y if x>0])\n",
    "df['id.Deco'].fillna(df['id.MrDeco']+200, inplace=True) # merge MrDeco ID into Deco IDs\n",
    "df['item_flag.Normal'].fillna(-1, inplace=True)\n",
    "df.rename(columns={'id.Deco':'deco_id',\n",
    "                   'enemy_flag.Em':'key_em_type.Em',\n",
    "                   'item_flag.Normal':'key_item_id',\n",
    "                   'item_id_list':'craft_item_id_list',\n",
    "                   'item_num_list':'craft_num_list'}, inplace=True)\n",
    "df_deco_flags = df[['deco_id','craft_item_id_list','craft_num_list','progress_flag','key_em_type.Em','key_item_id']].copy()\n",
    "df_deco_flags['deco_id'] = pd.to_numeric(df_deco_flags['deco_id'], errors='ignore', downcast='integer')\n",
    "df_deco_flags['key_item_id'] = pd.to_numeric(df_deco_flags['key_item_id'], errors='ignore', downcast='integer')\n",
    "\n",
    "# Rampage Decoration Unlock Conditions\n",
    "sect = raw.get('hyakuryu_decos_product').get('param')\n",
    "df = pd.json_normalize(sect)\n",
    "df['item_id_list'] = df['item_id_list'].apply(lambda y: [x[\"Normal\"] for x in y if isinstance(x,dict)])\n",
    "df['item_num_list'] = df['item_num_list'].apply(lambda y: [x for x in y if x>0])\n",
    "df['id.Deco'] = df['id.Deco']+400\n",
    "df['item_flag'].fillna(-1, inplace=True)\n",
    "df.rename(columns={'id.Deco':'deco_id',\n",
    "                   'enemy_flag.Em':'key_em_type.Em',\n",
    "                   'item_flag':'key_item_id',\n",
    "                   'item_id_list':'craft_item_id_list',\n",
    "                   'item_num_list':'craft_num_list'}, inplace=True)\n",
    "df_deco_flags = pd.concat([df_deco_flags,df[['deco_id','craft_item_id_list','craft_num_list','progress_flag','key_em_type.Em','key_item_id']]], axis=0, ignore_index=True)\n",
    "df_deco_flags['deco_id'] = pd.to_numeric(df_deco_flags['deco_id'], errors='ignore', downcast='integer')\n",
    "df_deco_flags['key_item_id'] = pd.to_numeric(df_deco_flags['key_item_id'], errors='ignore', downcast='integer')\n",
    "df_deco_flags.reset_index(drop=True, inplace=True)\n",
    "# print(df_deco_flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoding deco flags\n",
    "df = df_deco_flags\n",
    "df = pd.merge(left=df, left_on='progress_flag', right=df_prog_flags, right_on='progress_flag', how='left')\n",
    "df.drop(columns=['progress_flag'], inplace=True)\n",
    "df = pd.merge(left=df, left_on='key_item_id', right=df_items[['item_id','item_name']], right_on='item_id', how='left')\n",
    "df.drop(columns=['key_item_id','item_id'], inplace=True)\n",
    "df = pd.merge(left=df, left_on='key_em_type.Em', right=df_monsters[['em_type.Em','mon_name']], right_on='em_type.Em', how='left')\n",
    "df.drop(columns=['key_em_type.Em','em_type.Em'], inplace=True)\n",
    "df.fillna(-1, inplace=True)\n",
    "\n",
    "df = pd.merge(left=df, left_on='deco_id', right=df_deco_skills, right_on='deco_id', how='left')\n",
    "df = pd.merge(left=df, left_on='deco_id', right=df_deco_names, right_on='deco_id', how='left')\n",
    "df = pd.merge(left=df, left_on='skill_id', right=df_skill_names, right_on='skill_id', how='left')\n",
    "\n",
    "df_deco_unlock = df.copy()\n",
    "\n",
    "# df_deco_unlock.to_excel('deco_unlock.xlsx')\n",
    "# print(df_deco_unlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''Gathering Node Drop Tables'''\n",
    "# sect = raw.get('item_pop_lot').get('param')\n",
    "# df = pd.json_normalize(sect)\n",
    "# df['lower_item_ids'] = df['lower_id'].apply(lambda y: [x[\"Normal\"] for x in y if isinstance(x,dict)])\n",
    "# df['lower_item_ct'] = df['lower_num'].apply(lambda y: [x for x in y if x>0])\n",
    "# df['lower_item_chance'] = df['lower_probability'].apply(lambda y: [x for x in y if x>0])\n",
    "# df['upper_item_ids'] = df['upper_id'].apply(lambda y: [x[\"Normal\"] for x in y if isinstance(x,dict)])\n",
    "# df['upper_item_ct'] = df['upper_num'].apply(lambda y: [x for x in y if x>0])\n",
    "# df['upper_item_chance'] = df['upper_probability'].apply(lambda y: [x for x in y if x>0])\n",
    "# df['master_item_ids'] = df['master_id'].apply(lambda y: [x[\"Normal\"] for x in y if isinstance(x,dict)])\n",
    "# df['master_item_ct'] = df['master_num'].apply(lambda y: [x for x in y if x>0])\n",
    "# df['master_item_chance'] = df['master_probability'].apply(lambda y: [x for x in y if x>0])\n",
    "# df_item_pop_lot = df[['pop_id','field_type','lot_count','lower_item_ids','lower_item_ct', 'lower_item_chance','upper_item_ids','upper_item_ct', 'upper_item_chance','master_item_ids','master_item_ct', 'master_item_chance']]\n",
    "# print(df_item_pop_lot.head(5))\n",
    "# df_item_pop_lot.to_excel('gathering_node_drop_lot.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
